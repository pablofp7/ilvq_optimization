{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRUEBAS CON NAIVE BAYES DISTRIBUIDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from river.naive_bayes import GaussianNB\n",
    "from utils import read_dataset, evaluate_model_online_learning, calculate_metrics\n",
    "from collections import Counter\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from river.naive_bayes import GaussianNB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/ilvq_optimization/codigo/raspi/notebooks/utils.py:30: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dataset.replace({'UP': 1, 'DOWN': 0, 'True': 1, 'False': 0}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating models with interleaved samples...\n",
      "\n",
      "Metrics for node_1:\n",
      "  Accuracy: 0.626\n",
      "  Precision: 0.784\n",
      "  Recall: 0.626\n",
      "  F1-score: 0.696\n",
      "  Execution time: 0.09 seconds\n",
      "\n",
      "Metrics for node_2:\n",
      "  Accuracy: 0.663\n",
      "  Precision: 0.793\n",
      "  Recall: 0.663\n",
      "  F1-score: 0.722\n",
      "  Execution time: 0.09 seconds\n",
      "\n",
      "Metrics for node_3:\n",
      "  Accuracy: 0.611\n",
      "  Precision: 0.769\n",
      "  Recall: 0.611\n",
      "  F1-score: 0.681\n",
      "  Execution time: 0.10 seconds\n",
      "\n",
      "Evaluating Aggregated Model...\n",
      "\n",
      "Metrics for Aggregated Model:\n",
      "  Accuracy: 0.629\n",
      "  Precision: 0.888\n",
      "  Recall: 0.629\n",
      "  F1-score: 0.736\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of dataset names and file paths\n",
    "data_name = {\n",
    "    \"elec\": \"electricity.csv\",\n",
    "}\n",
    "\n",
    "# Load dataset\n",
    "name = \"elec\"\n",
    "dataset = read_dataset(name, data_name)\n",
    "dataset = dataset.iloc[:5000]  # Use a subset for faster testing\n",
    "\n",
    "# Separate features and labels\n",
    "feature_columns = dataset.columns[:-1]  # All except the last column\n",
    "label_column = dataset.columns[-1]      # The last column is the label\n",
    "\n",
    "\n",
    "# Split dataset into training and testing\n",
    "train_data = dataset.iloc[:3000]  # First 3000 samples for training\n",
    "test_data = dataset.iloc[3000:4000]  # Next 1000 samples for testing\n",
    "\n",
    "# Initialize models for each node\n",
    "node_models = {\n",
    "    \"node_1\": GaussianNB(),\n",
    "    \"node_2\": GaussianNB(),\n",
    "    \"node_3\": GaussianNB(),\n",
    "}\n",
    "\n",
    "# Train and evaluate each model using interleaved samples\n",
    "print(\"Training and evaluating models with interleaved samples...\")\n",
    "for node_idx, (node_name, model) in enumerate(node_models.items()):\n",
    "    interleaved_data = train_data.iloc[node_idx::3]  # Interleaved samples\n",
    "    conf_matrix, elapsed_time = evaluate_model_online_learning(model, interleaved_data)\n",
    "    metrics = calculate_metrics(conf_matrix)\n",
    "    print(f\"\\nMetrics for {node_name}:\")\n",
    "    print(f\"  Accuracy: {conf_matrix['TP'] / (conf_matrix['TP'] + conf_matrix['FN']):.3f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.3f}\")\n",
    "    print(f\"  F1-score: {metrics['f1']:.3f}\")\n",
    "    print(f\"  Execution time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Predict with the aggregated model using probabilities\n",
    "def predict_with_proba_aggregation(models, X):\n",
    "    \"\"\"\n",
    "    Predicts classes using aggregated probabilities across all models.\n",
    "\n",
    "    Args:\n",
    "        models (list): List of trained GaussianNB models.\n",
    "        X (list of dict): Test dataset (list of feature dictionaries).\n",
    "\n",
    "    Returns:\n",
    "        list: Predicted classes for each sample based on aggregated probabilities.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for xi in X:\n",
    "        # Aggregate probabilities\n",
    "        avg_proba = Counter()\n",
    "        for model in models:\n",
    "            proba = model.predict_proba_one(xi)\n",
    "            for class_label, p in proba.items():\n",
    "                avg_proba[class_label] += p\n",
    "        # Normalize and select the class with the highest probability\n",
    "        total_models = len(models)\n",
    "        aggregated_proba = {k: v / total_models for k, v in avg_proba.items()}\n",
    "        best_class = max(aggregated_proba, key=aggregated_proba.get)\n",
    "        predictions.append(best_class)\n",
    "    return predictions\n",
    "\n",
    "# Evaluate the aggregated model on the test dataset\n",
    "print(\"\\nEvaluating Aggregated Model...\")\n",
    "conf_matrix_agg = {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"TN\": 0}\n",
    "\n",
    "for _, row in test_data.iterrows():\n",
    "    x = {i: row[col] for i, col in enumerate(feature_columns)}\n",
    "    y = row[label_column]\n",
    "\n",
    "    # Predict using aggregated probabilities\n",
    "    y_pred = predict_with_proba_aggregation(list(node_models.values()), [x])[0]\n",
    "\n",
    "    # Update confusion matrix\n",
    "    if y == y_pred:\n",
    "        if y == 1:\n",
    "            conf_matrix_agg[\"TP\"] += 1\n",
    "        else:\n",
    "            conf_matrix_agg[\"TN\"] += 1\n",
    "    else:\n",
    "        if y == 1:\n",
    "            conf_matrix_agg[\"FN\"] += 1\n",
    "        else:\n",
    "            conf_matrix_agg[\"FP\"] += 1\n",
    "\n",
    "# Calculate metrics for the aggregated model\n",
    "metrics_agg = calculate_metrics(conf_matrix_agg)\n",
    "accuracy_agg = conf_matrix_agg[\"TP\"] / (conf_matrix_agg[\"TP\"] + conf_matrix_agg[\"FN\"])\n",
    "print(\"\\nMetrics for Aggregated Model:\")\n",
    "print(f\"  Accuracy: {accuracy_agg:.3f}\")\n",
    "print(f\"  Precision: {metrics_agg['precision']:.3f}\")\n",
    "print(f\"  Recall: {metrics_agg['recall']:.3f}\")\n",
    "print(f\"  F1-score: {metrics_agg['f1']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
