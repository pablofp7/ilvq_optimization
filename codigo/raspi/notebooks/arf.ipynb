{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRUEBAS CON ARF PARA ENTORNO DISTRIBUIDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import forest\n",
    "from utils import read_dataset, evaluate_model_online_learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset, i.e., have available the dataframe that corresponds to the dataset available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11160/228609128.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dataset.replace({'UP': 1, 'DOWN': 0, 'True': 1, 'False': 0}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of dataset names and file paths\n",
    "data_name = {\n",
    "    \"elec\": \"electricity.csv\",\n",
    "}\n",
    "\n",
    "# Load dataset\n",
    "name = \"elec\"\n",
    "dataset = read_dataset(name, data_name)\n",
    "dataset = dataset.iloc[:5000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARF PARAMETERS\n",
    "Tras las pruebas se ha visto que el que tiene un mejor equilibrio entre tiempo de ejecucion y rendimiento es con los parametros:\n",
    "- n_models = 3\n",
    "- max_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones auxiliares del modelo, extracción de parámetros, agregacion de parámetros etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_parameters(tree):\n",
    "    \"\"\"\n",
    "    Extracts the parameters of a single Hoeffding Tree for sharing with neighbors.\n",
    "\n",
    "    Args:\n",
    "        tree: The Hoeffding Tree Classifier.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary representing the tree's parameters (nodes, splits, and statistics).\n",
    "    \"\"\"\n",
    "    def traverse(node):\n",
    "        if node is None:\n",
    "            return None\n",
    "        \n",
    "        # Extract information for the current node\n",
    "        node_data = {\n",
    "            \"is_leaf\": node.is_leaf(),\n",
    "            \"split_feature\": getattr(node, \"split_feature\", None),\n",
    "            \"split_threshold\": getattr(node, \"split_threshold\", None),\n",
    "            \"class_counts\": getattr(node, \"class_counts\", None),\n",
    "            \"left_child\": None,\n",
    "            \"right_child\": None,\n",
    "        }\n",
    "        \n",
    "        # Recursively extract child nodes\n",
    "        if not node.is_leaf():\n",
    "            node_data[\"left_child\"] = traverse(node.left_child)\n",
    "            node_data[\"right_child\"] = traverse(node.right_child)\n",
    "        \n",
    "        return node_data\n",
    "\n",
    "    # Start traversal from the root\n",
    "    tree_params = traverse(tree.root)\n",
    "    return {\"nodes\": tree_params}\n",
    "\n",
    "\n",
    "def aggregate_parameters(tree, received_params):\n",
    "    \"\"\"\n",
    "    Aggregates the local Hoeffding Tree parameters with received parameters.\n",
    "\n",
    "    Args:\n",
    "        tree: The local Hoeffding Tree Classifier.\n",
    "        received_params: A list of parameters received from neighbors.\n",
    "\n",
    "    Returns:\n",
    "        The updated Hoeffding Tree Classifier.\n",
    "    \"\"\"\n",
    "    def merge_nodes(local_node, neighbor_nodes):\n",
    "        if local_node is None or all(n is None for n in neighbor_nodes):\n",
    "            return local_node\n",
    "\n",
    "        # Merge class counts (leaf nodes)\n",
    "        if local_node[\"is_leaf\"]:\n",
    "            for neighbor_node in neighbor_nodes:\n",
    "                if neighbor_node and neighbor_node[\"is_leaf\"]:\n",
    "                    for class_label, count in neighbor_node[\"class_counts\"].items():\n",
    "                        local_node[\"class_counts\"][class_label] = (\n",
    "                            local_node[\"class_counts\"].get(class_label, 0) + count\n",
    "                        ) / (len(neighbor_nodes) + 1)\n",
    "\n",
    "        # Merge split statistics (non-leaf nodes)\n",
    "        if not local_node[\"is_leaf\"]:\n",
    "            local_node[\"left_child\"] = merge_nodes(\n",
    "                local_node[\"left_child\"],\n",
    "                [n[\"left_child\"] for n in neighbor_nodes if n and not n[\"is_leaf\"]],\n",
    "            )\n",
    "            local_node[\"right_child\"] = merge_nodes(\n",
    "                local_node[\"right_child\"],\n",
    "                [n[\"right_child\"] for n in neighbor_nodes if n and not n[\"is_leaf\"]],\n",
    "            )\n",
    "\n",
    "        return local_node\n",
    "\n",
    "    # Aggregate received parameters into the local tree\n",
    "    local_tree_params = subtract_parameters(tree)[\"nodes\"]\n",
    "    neighbor_tree_params = [params[\"nodes\"] for params in received_params]\n",
    "\n",
    "    # Merge the root node and update the local tree\n",
    "    merged_root = merge_nodes(local_tree_params, neighbor_tree_params)\n",
    "    tree.root = merged_root\n",
    "\n",
    "    return tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTINUAR AQUÍ, REWORKED FUNCIONES DE AGREGACION Y EXTRACCION DE PARÁMETROS\n",
    "\n",
    "\n",
    "#### Vamos a entrenar dos modelos con muestras intercaladas ya que es un dataset con concept drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Evaluation Results:\n",
      "Modelo1 Confusion Matrix: {'TP': 792, 'TN': 1320, 'FP': 166, 'FN': 222}\n",
      "Modelo1 Execution Time: 0.92 seconds\n",
      "Modelo2 Confusion Matrix: {'TP': 688, 'TN': 1390, 'FP': 176, 'FN': 246}\n",
      "Modelo2 Execution Time: 0.92 seconds\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into interleaved chunks\n",
    "chunk1 = dataset.iloc[::2].reset_index(drop=True)  # Even-indexed samples (0, 2, 4, ...)\n",
    "chunk2 = dataset.iloc[1::2].reset_index(drop=True)  # Odd-indexed samples (1, 3, 5, ...)\n",
    "\n",
    "# Initialize the ARF models\n",
    "modelo1 = forest.ARFClassifier(n_models=3, max_size=20)\n",
    "modelo2 = forest.ARFClassifier(n_models=3, max_size=20)\n",
    "\n",
    "# Train modelo1 on chunk1 and modelo2 on chunk2\n",
    "conf_matrix1, elapsed_time1 = evaluate_model_online_learning(modelo1, chunk1)\n",
    "conf_matrix2, elapsed_time2 = evaluate_model_online_learning(modelo2, chunk2)\n",
    "\n",
    "# Print initial evaluation results\n",
    "print(\"Initial Evaluation Results:\")\n",
    "print(f\"Modelo1 Confusion Matrix: {conf_matrix1}\")\n",
    "print(f\"Modelo1 Execution Time: {elapsed_time1:.2f} seconds\")\n",
    "print(f\"Modelo2 Confusion Matrix: {conf_matrix2}\")\n",
    "print(f\"Modelo2 Execution Time: {elapsed_time2:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a printear los parámetros extraídos y antes y después de agregar para comprobar que la agregación se está realizando correctamente y que ambos tienen los mismos parámetros (promedio de los que tenian antes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ForestMemberClassifier' object has no attribute 'iter_nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m params1 \u001b[38;5;241m=\u001b[39m \u001b[43msubtract_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelo1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m params2 \u001b[38;5;241m=\u001b[39m subtract_parameters(modelo2)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPametros modelo1:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m, in \u001b[0;36msubtract_parameters\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     13\u001b[0m tree_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],  \u001b[38;5;66;03m# Store node information\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],  \u001b[38;5;66;03m# Store split information\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatistics\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],  \u001b[38;5;66;03m# Store statistics (e.g., class distributions)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m }\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Traverse the tree and extract node, split, and statistics information\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_nodes\u001b[49m():\n\u001b[1;32m     20\u001b[0m     tree_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(node)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mis_split_node():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ForestMemberClassifier' object has no attribute 'iter_nodes'"
     ]
    }
   ],
   "source": [
    "params1 = subtract_parameters(modelo1)\n",
    "params2 = subtract_parameters(modelo2)\n",
    "\n",
    "print(f\"Pametros modelo1:\\n {params1}\\n\\n\\n\")\n",
    "print(f\"Pametros modelo1:\\n {params2}\\n\\n\\n\")\n",
    " \n",
    "model1_aggregated = aggregate_parameters(modelo1, params2)\n",
    "model2_aggregated = aggregate_parameters(modelo2, params1)\n",
    "\n",
    "params_aggregated1 = subtract_parameters(model1_aggregated)\n",
    "params_aggregated2 = subtract_parameters(model2_aggregated)\n",
    "\n",
    "print(f\"Pametros modelo1 agregados:\\n {params_aggregated1}\\n\\n\\n\")\n",
    "print(f\"Pametros modelo2 agregados:\\n {params_aggregated2}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
